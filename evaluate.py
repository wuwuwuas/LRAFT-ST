import scipy.signalimport osimport argparseimport torchimport torch.nn.functional as Ffrom tqdm import tqdmimport mathimport numpy as npimport importlibimport timefrom RMSE_AAE import rmse, aeeimport torch.nn as nnimport os.pathimport globfrom PIL import Imageimport scipy.io as sciofrom torch.utils.data import TensorDataset, DataLoaderimport hdf5storage##############################################################################################################################################################def _spline_window(window_size, power=2):    """    Squared spline (power=2) window function:    https://www.wolframalpha.com/input/?i=y%3Dx**2,+y%3D-(x-2)**2+%2B2,+y%3D(x-4)**2,+from+y+%3D+0+to+2    """    intersection = int(window_size/4)    wind_outer = (abs(2*(scipy.signal.triang(window_size))) ** power)/2    wind_outer[intersection:-intersection] = 0    wind_inner = 1 - (abs(2*(scipy.signal.triang(window_size) - 1)) ** power)/2    wind_inner[:intersection] = 0    wind_inner[-intersection:] = 0    wind = wind_inner + wind_outer    wind = wind / np.average(wind)    return wind#spline windowingcached_2d_windows = dict()def _window_2D(window_size, power=2):    """    Make a 1D window function, then infer and return a 2D window function.    Done with an augmentation, and self multiplication with its transpose.    Could be generalized to more dimensions.    """    # Memoization    global cached_2d_windows    key = "{}_{}".format(window_size, power)    if key in cached_2d_windows:        wind = cached_2d_windows[key]    else:        wind = _spline_window(window_size, power)        wind = np.expand_dims(np.expand_dims(wind, -1), -1)        wind = wind * wind.transpose(1, 0, 2)        cached_2d_windows[key] = wind    return windtry:    from torch.cuda.amp import GradScalerexcept:    # dummy GradScaler for PyTorch < 1.6    class GradScaler:        def __init__(self):            pass        def scale(self, loss):            return loss        def unscale_(self, optimizer):            pass        def step(self, optimizer):            optimizer.step()        def update(self):            passtry:    autocast = torch.cuda.amp.autocastexcept:    # dummy autocast for PyTorch < 1.6    class autocast:        def __init__(self, enabled):            pass        def __enter__(self):            pass        def __exit__(self, *args):            passclass WrappedModel(nn.Module):    def __init__(self, model, args):        super(WrappedModel, self).__init__()        self.model = model        self.args = args    def forward(self, x):        return self.model(x, self.args)### main methoddef main():    parser = argparse.ArgumentParser()    parser.add_argument('-g', '--gpu', default=0, type=int,                        help='index of gpu used')    parser.add_argument('--input_path_ckpt', type=str, default='./precomputed_ckpts/LRAFT-ST_KD_Lh_Ls_Lf-Dataset2/ckpt.tar',                        help='path of already trained checkpoint')    parser.add_argument('--channel_threshold', type=str, default=True,                        choices=['False', 'True'],                        help='Wether to use channel wise threshold')    parser.add_argument('--test', type=int, default=1, choices=[0, 1],                        help='whether the evaluation mode is used, 1 yes, 0 no')    parser.add_argument('--amp', type=eval, default=False,                        help='Wether to use auto mixed precision')    parser.add_argument('--test_dataset', type=str, default='PIV_4B',                        choices=['back_step', 'cylinder', 'jhtdb_c', 'dns_turb', 'sqg', 'uniform',                                 'jhtdb_c_hd', 'jhtdb_iso_hd', 'jhtdb_mhd_hd', 'dataset2', 'PIV_4B'],                        help='test dataset to evaluate')    parser.add_argument('-a', '--arch', type=str, default='LRAFT-ST',                        choices=['LightPIVNet', 'RAFT', 'RAFT_4', 'RAFT_4-ST', 'LRAFT', 'LRAFT-ST'],                        help='Type of architecture to use')    parser.add_argument('--iters', default=12, type=int,                        help='number of update steps in ConvGRU')    parser.add_argument('--upsample', type=str, default='convex',                        choices=['convex'],                        help="""Type of upsampling method""")    parser.add_argument('--save_path', type=str, default='./results/')    parser.add_argument('--split_size', default=5, type=int)    parser.add_argument('--batch_size', default=1, type=int)    parser.add_argument('--offset', default=256, type=int,                        help='interrogation window size')    parser.add_argument('--shift', default=64, type=int,                        help='shift of interrogation windon in px')    args = parser.parse_args()    print('args parsed')    np.random.seed(0)    torch.manual_seed(0)    torch.set_grad_enabled(False)    evaluate(args)def evaluate(args):    device = torch.device(f'cuda:{args.gpu}')    if args.arch == 'LRAFT-ST' or args.arch == 'LRAFT':        module_name = 'LRAFT-ST.RAFT_threshold'        model_name = 'LRAFT_ST'        module = importlib.import_module(module_name)        LRAFT_ST = getattr(module, model_name)        model = LRAFT_ST(args)        print('Selected model: LRAFT - -', args.arch, '  Channel threshold- -', args.channel_threshold)    elif args.arch == 'RAFT':        module_name = 'RAFT.flowNetsRAFT256'        model_name = 'RAFT256'        module = importlib.import_module(module_name)        RAFT256 = getattr(module, model_name)        model = RAFT256(args)        print('Selected model: RAFT - -', args.arch)    elif args.arch == 'RAFT_4' or args.arch == 'RAFT_4-ST':        module_name = 'RAFT_4-ST.flowNetsRAFT256'        model_name = 'RAFT256'        module = importlib.import_module(module_name)        RAFT256 = getattr(module, model_name)        model = RAFT256(args)        print('Selected model: RAFT 1/4 resolution- -', args.arch, '  Channel threshold- -', args.channel_threshold)    elif args.arch == 'LightPIVNet':        module_name = 'LightPIVNet.raft'        model_name = 'LightPIVNet'        module = importlib.import_module(module_name)        LightPIVNet = getattr(module, model_name)        model = LightPIVNet(args)        print('Selected model: LightPIVNet - -', args.arch)    else:        raise ValueError('Selected model not supported: ', args.arch)            checkpoint = torch.load(args.input_path_ckpt,map_location=device)    model.load_state_dict(checkpoint['model_state_dict'])    model.to(device)    print('Test Case:', args.test_dataset)    if args.test_dataset in ['back_step', 'cylinder', 'dns_turb', 'sqg', 'uniform', 'jhtdb_c', 'jhtdb_c_hd', 'jhtdb_iso_hd', 'jhtdb_mhd_hd', 'dataset2']:        if args.test_dataset == 'back_step':            test_image_path = './data/Dataset1/test_image_backstep_10.mat'            test_flow_path = './data/Dataset1/test_flow_backstep_10.mat'        elif args.test_dataset == 'cylinder':            test_image_path = './data/Dataset1/test_image_cylinder_10.mat'            test_flow_path = './data/Dataset1/test_flow_cylinder_10.mat'        elif args.test_dataset == 'dns_turb':            test_image_path = './data/Dataset1/test_image_DNS_turbulence_10.mat'            test_flow_path = './data/Dataset1/test_flow_DNS_turbulence_10.mat'        elif args.test_dataset == 'sqg':            test_image_path = './data/Dataset1/test_image_SQG_10.mat'            test_flow_path = './data/Dataset1/test_flow_SQG_10.mat'        elif args.test_dataset == 'uniform':            test_image_path = './data/Dataset1/test_image_uniform_10.mat'            test_flow_path = './data/Dataset1/test_flow_uniform_10.mat'        elif args.test_dataset == 'jhtdb_c':            test_image_path = './data/Dataset1/test_image_JHTDB_channel_10.mat'            test_flow_path = './data/Dataset1/test_flow_JHTDB_channel_10.mat'        elif args.test_dataset == 'jhtdb_c_hd':            test_image_path = './data/Dataset1/test_image_JHTDB_channel_hd_10.mat'            test_flow_path = './data/Dataset1/test_flow_JHTDB_channel_hd_10.mat'        elif args.test_dataset == 'jhtdb_iso_hd':            test_image_path = './data/Dataset1/test_image_JHTDB_isotropic1024_hd_10.mat'            test_flow_path = './data/Dataset1/test_flow_JHTDB_isotropic1024_hd_10.mat'        elif args.test_dataset == 'jhtdb_mhd_hd':            test_image_path = './data/Dataset1/test_image_JHTDB_mhd1024_hd_10.mat'            test_flow_path = './data/Dataset1/test_flow_JHTDB_mhd1024_hd_10.mat'        elif args.test_dataset == 'dataset2':            test_image_path = './data/Dataset2/test_image_20.mat'            test_flow_path = './data/Dataset2/test_flow_20.mat'        test_image = hdf5storage.loadmat(test_image_path)        test_flow = hdf5storage.loadmat(test_flow_path)        test_image = torch.from_numpy(test_image['image']).to(torch.float32) / 255        test_flow = torch.from_numpy(test_flow['flow']).to(torch.float32)        print(test_image.shape)        test_data = TensorDataset(test_image[:, :, :, :],test_flow[:, :, :, :])        test_data = DataLoader(test_data, batch_size = args.batch_size, shuffle = True)        last_training_loss, last_validation_loss = 0.0, 0.0        test_rmse = []        test_aee = []        # Validation        with torch.set_grad_enabled(False):            #set evaluation mode            model.eval()            val_loader_len = int(math.ceil(len(test_data) / args.batch_size))            val_pbar = tqdm(enumerate(test_data), total=val_loader_len,                        desc='Epoch: [' + str(1+1) + '/' + str(1) + '] Validation',                        postfix='loss: ' + str(last_validation_loss), position=1, leave=False)            for i, sample_batched in val_pbar:                images, flows = sample_batched                fflows = flows                B, C, H, W = images.size()                NUM_Yvectors, NUM_Xvectors = int(H / args.shift - (args.offset / args.shift - 1)), int(W / args.shift - (args.offset / args.shift - 1))                predicted_flows = torch.zeros_like(images).to(device)                patches = images.unfold(3, args.offset, args.shift).unfold(2, args.offset, args.shift).permute(0, 2, 3,1, 5, 4)                patches = patches.reshape((-1, 2, args.offset, args.offset))                splitted_patches = torch.split(patches, args.split_size, dim=0)                folding_mask = torch.ones_like(flows).to(device)                splitted_flow_output_patches = []                per_t1 = time.time()                for split in range(len(splitted_patches)):                    images = splitted_patches[split].type(torch.FloatTensor).to(device)                    with autocast(enabled=args.amp):                        pred_flows = model(images, args=args)                        all_flow_iters = pred_flows                        splitted_flow_output_patches.append(all_flow_iters[-1])                per_t2 = time.time()                print("time for per sample:", per_t2-per_t1)                flow_output_patches = torch.cat(splitted_flow_output_patches, dim=0)                WINDOW_SPLINE_2D = torch.from_numpy(np.squeeze(_window_2D(window_size=args.offset, power=2)))                flow_output_patches = flow_output_patches * WINDOW_SPLINE_2D.to(device)                flow_output_patches = flow_output_patches.reshape((B, NUM_Yvectors, NUM_Xvectors, 2, args.offset, args.offset)). \                    permute(0, 3, 1, 2, 4, 5)                flow_output_patches = flow_output_patches.contiguous().view(B, C, -1, args.offset * args.offset)                flow_output_patches = flow_output_patches.permute(0, 1, 3, 2)                flow_output_patches = flow_output_patches.contiguous().view(B, C * args.offset * args.offset, -1)                predicted_flows_iter = F.fold(flow_output_patches, output_size=(H, W), kernel_size=args.offset, stride=args.shift)                mask_patches = folding_mask.unfold(3, args.offset, args.shift).unfold(2, args.offset, args.shift)                mask_patches = mask_patches.contiguous().view(B, C, -1, args.offset, args.offset)                mask_patches = mask_patches * WINDOW_SPLINE_2D.to(device)                mask_patches = mask_patches.view(B, C, -1, args.offset * args.offset)                mask_patches = mask_patches.permute(0, 1, 3, 2)                mask_patches = mask_patches.contiguous().view(B, C * args.offset * args.offset, -1)                folding_mask = F.fold(mask_patches, output_size=(H, W), kernel_size=args.offset, stride=args.shift)                predicted_flows += predicted_flows_iter / folding_mask                predicted_flows = predicted_flows.permute(0, 2, 3, 1)                flows = fflows.permute(0, 2, 3, 1).to(device)                RMSE = rmse(predicted_flows, flows)                AEE = aee(predicted_flows, flows)                test_rmse.append(RMSE.item())                test_aee.append(AEE.item())                print('RMSE:', RMSE.item(), 'AEE:', AEE.item(),'time:',per_t2-per_t1)            print('RMSE_ALL_MEAN:',np.mean(test_rmse), 'AEE_ALL_MEAN:', np.mean(test_aee))    elif args.test_dataset == 'PIV_4B':        test_image_path = './test_case/PIV_4B/*.tif'        args.shiftx = 64        args.shifty = 68        images = glob.glob(test_image_path)        image_pairs = []        for i in range(0,(len(images)-1),1):            imag1 = images[i]            imag2 = images[i + 1]            image_pairs.append([imag1, imag2])        save_path = args.save_path + args.arch + '/' + args.test_dataset + '/'        if not os.path.exists(save_path):            os.makedirs(save_path)        i = 0        test_image = []        for pair in image_pairs:            image1 = np.array(Image.open(pair[0]).convert('L'), dtype=np.float32)            image2 = np.array(Image.open(pair[1]).convert('L'), dtype=np.float32)            print(pair[0], pair[1], image1.max())            H, W = image1.shape            image1 = np.resize(image1, (int(H / 1), int(W / 1)))            image2 = np.resize(image2, (int(H / 1), int(W / 1)))            i = i + 1            target = np.stack([image1, image2], axis=0)            test_image.append(target)        test_image = np.array(test_image)        test_image = torch.from_numpy(test_image).to(torch.float32) / test_image.max()        print(test_image.shape)        # Validation        with torch.set_grad_enabled(False):            # set evaluation mode            model.eval()            for i in range(len(test_image)):                images = test_image[i:i + 1, :, :, :]                B, C, H, W = images.size()                NUM_Yvectors, NUM_Xvectors = int((H - args.offset) / args.shifty + 1), int(                    (W - args.offset) / args.shiftx + 1)                predicted_flows = torch.zeros_like(images).to(device)                patches = images.unfold(3, args.offset, args.shiftx).unfold(2, args.offset, args.shifty).permute(0, 2,                                                                                                                 3, 1,                                                                                                                 5, 4)                patches = patches.reshape((-1, 2, args.offset, args.offset))                splitted_patches = torch.split(patches, args.split_size, dim=0)                folding_mask = torch.ones_like(images).to(device)                splitted_flow_output_patches = []                for split in range(len(splitted_patches)):                    images = splitted_patches[split].type(torch.FloatTensor).to(device)                    with autocast(enabled=args.amp):                        pred_flows = model(images, args=args)                        all_flow_iters = pred_flows                        splitted_flow_output_patches.append(all_flow_iters[-1])                flow_output_patches = torch.cat(splitted_flow_output_patches, dim=0)                WINDOW_SPLINE_2D = torch.from_numpy(np.squeeze(_window_2D(window_size=args.offset, power=2)))                flow_output_patches = flow_output_patches * WINDOW_SPLINE_2D.to(device)                flow_output_patches = flow_output_patches.reshape(                    (B, NUM_Yvectors, NUM_Xvectors, 2, args.offset, args.offset)). \                    permute(0, 3, 1, 2, 4, 5)                flow_output_patches = flow_output_patches.contiguous().view(B, C, -1, args.offset * args.offset)                flow_output_patches = flow_output_patches.permute(0, 1, 3, 2)                flow_output_patches = flow_output_patches.contiguous().view(B, C * args.offset * args.offset, -1)                predicted_flows_iter = F.fold(flow_output_patches, output_size=(H, W), kernel_size=args.offset,                                              stride=(args.shifty, args.shiftx))                mask_patches = folding_mask.unfold(3, args.offset, args.shiftx).unfold(2, args.offset, args.shifty)                mask_patches = mask_patches.contiguous().view(B, C, -1, args.offset, args.offset)                mask_patches = mask_patches * WINDOW_SPLINE_2D.to(device)                mask_patches = mask_patches.view(B, C, -1, args.offset * args.offset)                mask_patches = mask_patches.permute(0, 1, 3, 2)                mask_patches = mask_patches.contiguous().view(B, C * args.offset * args.offset, -1)                folding_mask = F.fold(mask_patches, output_size=(H, W), kernel_size=args.offset,                                      stride=(args.shifty, args.shiftx))                predicted_flows += predicted_flows_iter / folding_mask                predicted_flows = predicted_flows.permute(0, 2, 3, 1)                predicted_flows = predicted_flows.cpu().detach().numpy()                scio.savemat(save_path + 'results' + str(i) + '.mat', {'pre': predicted_flows})                print(i)    else:        raise ValueError('Selected dataset not supported: ', args.test_dataset)    torch.cuda.empty_cache()if __name__ == '__main__':    main()